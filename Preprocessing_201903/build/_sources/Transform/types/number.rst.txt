.. Preprocessing documentation master file, created by
   sphinx-quickstart on Thu Mar 28 11:28:08 2019.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

.. _making-a-table:

number
==================

数値型変換
----------------------------------------
# 単一の値の型変換
# データ型の確認

.. code-block:: python

   type(40000 / 3)

# 整数型へ変換

.. code-block:: python

   int(40000 / 3)

# 浮動小数点型へ変換

.. code-block:: python

   float(40000 / 3)

# dfオブジェクトの型変換
# データ型の確認

.. code-block:: python

   df = pd.DataFrame({'value': [40000 / 3]})
   df.dtypes

# データ型指定

.. code-block:: python

   df['value'].astype(int)  # int64
   df['value'].astype(float)  # float64

# bit明示的な変換
# サポートされているbit数は以下の通り
# int/uint 8,16,32,64
# float      16,32,64,128

.. code-block:: python

   df['value'].astype('int8')
   df['value'].astype('float128')

対数化
----------------------------------------
.. code-block:: python

   df['price'] = \
      df['price'].apply(lambda x: np.log(x / 1000 + 1))

カテゴリ変数化
----------------------------------------
.. code-block:: python

   df['age'] = \
      (np.floor(df['age'] / 10) * 10).astype('category')

正規化
----------------------------------------
.. code-block:: python

   # 少数点以下を扱えるようにするためfloat型に変換
   df['count'] = df['count'].astype(float)

   # 正規化を行うオブジェクトを生成
   ss = StandardScaler()

   # fit_transform関数は、fit関数（正規化するための前準備の計算）と
   # transform関数（準備された情報から正規化の変換処理を行う）の両方を行う
   result = ss.fit_transform(df[['count', 'price']])

   df['count_num_normalized'] = [x[0] for x in result]
   df['price_normalized'] = [x[1] for x in result]


外れ値除去（平均値±3σ以内のデータのみ抽出）
--------------------------------------------------------------------------------
.. code-block:: python

   df = df[
     (abs(df['price'] - np.mean(df['price'])) /
      np.std(df['price']) <= 3)
   ].reset_index()

次元圧縮
----------------------------------------
.. code-block:: python

   # n_componentsに、主成分分析で変換後の次元数を設定
   pca = PCA(n_components=2)

   # 主成分分析を実行
   # pcaに主成分分析の変換パラメータが保存され、返り値に主成分分析後の値が返される
   pca_values = pca.fit_transform(df[['length', 'thickness']])

   # 累積寄与率と寄与率の確認
   print('累積寄与率: {0}'.format(sum(pca.explained_variance_ratio_)))
   print('各次元の寄与率: {0}'.format(pca.explained_variance_ratio_))

   # predict関数を利用し、同じ次元圧縮処理を実行
   pca_newvalues = pca.transform(df[['length', 'thickness']])


補完/欠損値処理
----------------------------------------
# 特定の値で補完
# replace関数によって、Noneをnanに変換
# （Noneを指定する際には文字列として指定する必要がある）

.. code-block:: python

   df.replace('None', np.nan, inplace=True)

# dropna関数によって、thicknessにnanを含むレコードを削除

.. code-block:: python

   df.dropna(subset=['thickness'], inplace=True)

# fillna関数によって、thicknessの欠損値を1で補完

.. code-block:: python

   production_miss_num['thickness'].fillna(1, inplace=True)

# 平均値で補完
# thicknessの平均値を計算

.. code-block:: python

   thickness_mean = df['thickness'].astype('float64').mean()

# thicknessの欠損値をthicknessの平均値で補完

.. code-block:: python

   df['thickness'].fillna(thickness_mean, inplace=True)


# 連鎖式による多重代入法
# replace関数によって、Noneをnanに変換

.. code-block:: python

   df.replace('None', np.nan, inplace=True)

   # mice関数を利用するためにデータ型を変換（mice関数内でモデル構築をするため）
   df['thickness'] = df['thickness'].astype('float64')
   df['type'] = df['type'].astype('category')
   df['fault_flg'] = df['fault_flg'].astype('category')

   # ダミー変数化
   df_dummy_flg = pd.get_dummies(df[['type', 'fault_flg']], drop_first=True)

   # mice関数にPMMを指定して、多重代入法を実施
   # n_imputationsは取得するデータセットの数
   # n_burn_inは値を取得する前に試行する回数
   mice = MICE(n_imputations=10, n_burn_in=50, impute_type='pmm')

   # 処理内部でTensorFlowを利用
   df_mice = mice.multiple_imputations(
       # 数値の列とダミー変数を連結
      pd.concat([df[['length', 'thickness']],
                df_dummy_flg], axis=1)
   )

   # 下記に補完する値が格納されている
   df_mice[0]


サンプルコード

.. literalinclude:: ../../../transform/types/number_transformer.py

